# -*- coding: utf-8 -*-
"""project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vt40K7opLuVH6VG0kYw8Obg-iCqn-cjH
"""

pip install tensorflow_lattice

import numpy as np
import tensorflow as tf
import tensorflow.keras as keras
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from scipy.stats import pearsonr
import tensorflow_lattice as tfl

from google.colab import drive
drive.mount('/content/drive')

data = pd.read_csv('/content/drive/MyDrive/EURUSD.csv')
# data = pd.read_csv('/content/drive/MyDrive/EURGBP.csv')

data = data.set_index(pd.to_datetime(data['Time']))
data = data.drop(columns=['Time'])
data = data.drop(columns=['Volume'])

print(data.columns)
my_dataset = data.to_csv('dataset.csv')

print(data)

def createDS (daily_ds, horizen = 1 , lag = 22):
  daily_ds = daily_ds.transpose()
  x_eurusd = []
  y_eurusd = []
  features =[]
  ds = daily_ds.to_numpy()
  for i in range (daily_ds.shape[1] - lag - horizen) :
    for j in range (daily_ds.shape[0]):
      features.append(ds[j, i : i + lag])
    x_eurusd.append(np.transpose(features))
    y_eurusd.append(ds[3,i-1 + lag + horizen])
    features= []
  return x_eurusd , y_eurusd

X , Y = createDS(data, 1, 22)

x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, shuffle = False, random_state = 0)

print(np.shape(x_test))
print(np.shape(x_train))

x_test = x_test[0:900]
y_test = y_test[0:900]
x_train = x_train[37:3637]
y_train = y_train[37:3637]

print(np.shape(x_test))
print(np.shape(x_train))

x_train = np.array(x_train)
x_test = np.array(x_test)

def data_scalar(x_train, x_test):
  scalers = {}
  for i in range(x_train.shape[2]):
    scalers[i] = StandardScaler()
    x_train[:, :, i] = scalers[i].fit_transform(x_train[:, :, i])

  for i in range(x_test.shape[2]):
    x_test[:, :, i] = scalers[i].transform(x_test[:, :, i])

  return x_train, x_test

x_train, x_test = data_scalar(x_train, x_test)

x_train = np.array(x_train).astype('float32')
y_train = np.array(y_train).astype('float32')

input = tf.keras.layers.Input(shape =(22, 4,), batch_size=10)

firstConv = tf.keras.layers.Conv1D(32, 4, activation = 'relu')(input)
firstPooling = tf.keras.layers.MaxPooling1D(pool_size=4, strides=2)(firstConv)

secondConv = tf.keras.layers.Conv1D(128, 2, activation ='relu')(firstPooling)
secondPooling = tf.keras.layers.MaxPooling1D(pool_size=4, strides=2)(secondConv)

fisrtLSTM = tf.keras.layers.LSTM(10)(input)
secondLSTM = tf.keras.layers.LSTM(20)(firstPooling)
thirdLSTM = tf.keras.layers.LSTM(30)(secondPooling)

concat = tf.keras.layers.concatenate([fisrtLSTM, secondLSTM, thirdLSTM])

lin = tfl.layers.Linear(num_input_dims=60,
    # Monotonicity constraints can be defined per dimension or for all dims.
    monotonicities='increasing',
    use_bias=True,
    # You can force the L1 norm to be 1. Since this is a monotonic layer,
    # the coefficients will sum to 1, making this a "weighted average".
    normalization_order=1)(concat)

firstFC = tf.keras.layers.Dense(100)(concat)
secondFC = tf.keras.layers.Dense(50)(firstFC)
thirdFC = tf.keras.layers.Dense(1)(secondFC)

model = tf.keras.Model(inputs = input, outputs = thirdFC)

model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss=keras.losses.MSE, metrics=['mse'])

callback = tf.keras.callbacks.EarlyStopping(monitor="loss", patience=20, restore_best_weights=True,)

model.summary()

history = model.fit(x_train, y_train, epochs=76, batch_size=10, validation_split=0.1, callbacks=callback)
epochs = history.epoch
hist = pd.DataFrame(history.history)
train_mse = hist["loss"]
val_mse = hist["val_loss"]
plt.plot(epochs, train_mse)
plt.plot(epochs, val_mse)
plt.show()

tf.keras.utils.plot_model(model, "my_model.png", show_shapes=True)

x_test = np.asarray(np.array(x_test)).astype('float32')
y_test = np.asarray(np.array(y_test)).astype('float32')

model.evaluate(x = x_test, y = y_test, batch_size=10)

predictions = model.predict(x_test, batch_size=10)

model.save('my_model.h5')

x = np.arange(100)
plt.plot(x[0:100], predictions[0:100])
plt.plot(x[0:100], y_test[0:100])
plt.show()

x = np.arange(200)
plt.plot(x[0:200], predictions[0:200])
plt.plot(x[0:200], y_test[0:200])
plt.show()

x = np.arange(20)
plt.plot(x[0:20], predictions[0:20])
plt.plot(x[0:20], y_test[0:20])
plt.show()

x = np.arange(400)
plt.plot(x[0:400], predictions[0:400])
plt.plot(x[0:400], y_test[0:400])
plt.show()

x = np.arange(900)
plt.plot(x[0:900], predictions[0:900])
plt.plot(x[0:900], y_test[0:900])
plt.show()